# ============================
# train_age_resume.py ‚Äì Train + Resume Age Model (FULL VERSION)
# ============================

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import os
import json

from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error


# =======================
# T·∫°o th∆∞ m·ª•c
# =======================
os.makedirs("plots", exist_ok=True)
os.makedirs("checkpoints", exist_ok=True)

CHECKPOINT_PATH = "checkpoints/best_age_model.keras"
HISTORY_PATH = "checkpoints/history.json"


# =======================
# Load d·ªØ li·ªáu NPZ
# =======================
data = np.load(r"C:\Users\phuoc\Downloads\GenderAgeAI\data\processed\utkface_preprocessed.npz")
X = data['X']
ages = data['age']

# =======================
# Train/Val/Test Split
# =======================
X_train, X_testval, y_train, y_testval = train_test_split(X, ages, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_testval, y_testval, test_size=0.5, random_state=42)

print("Train:", X_train.shape, "Val:", X_val.shape, "Test:", X_test.shape)


# =======================
# Data augmentation
# =======================
datagen_train = ImageDataGenerator(
    rotation_range=10,
    width_shift_range=0.05,
    height_shift_range=0.05,
    zoom_range=0.1,
    horizontal_flip=True
)

datagen_val = ImageDataGenerator()

batch_size = 64
train_gen = datagen_train.flow(X_train, y_train, batch_size=batch_size)
val_gen = datagen_val.flow(X_val, y_val, batch_size=batch_size)



# =======================
# Build Age Model
# =======================
def build_age_model(input_shape):
    base = MobileNetV2(input_shape=input_shape, include_top=False, weights="imagenet")
    x = GlobalAveragePooling2D()(base.output)
    x = Dense(256, activation="relu")(x)
    x = Dropout(0.3)(x)
    output = Dense(1)(x)
    model = Model(base.input, output)
    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),
                  loss="mse", metrics=["mae"])
    return model


# =======================
# Callback: L∆∞u epoch
# =======================
class EpochSaver(tf.keras.callbacks.Callback):
    """L∆∞u epoch hi·ªán t·∫°i ƒë·ªÉ l·∫ßn sau ti·∫øp t·ª•c."""
    def on_epoch_end(self, epoch, logs=None):
        with open("checkpoints/epoch.txt", "w") as f:
            f.write(str(epoch + 1))


# =======================
# Callback: L∆∞u to√†n b·ªô l·ªãch s·ª≠ train
# =======================
class HistorySaver(tf.keras.callbacks.Callback):
    """L∆∞u l·ªãch s·ª≠ train/val v√†o history.json ƒë·ªÉ gh√©p bi·ªÉu ƒë·ªì."""
    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}

        # N·∫øu file ch∆∞a c√≥ th√¨ t·∫°o m·ªõi
        if os.path.exists(HISTORY_PATH):
            with open(HISTORY_PATH, "r") as f:
                history = json.load(f)
        else:
            history = {"loss": [], "val_loss": [], "mae": [], "val_mae": []}

        # Th√™m log m·ªõi
        history["loss"].append(float(logs.get("loss", 0)))
        history["val_loss"].append(float(logs.get("val_loss", 0)))
        history["mae"].append(float(logs.get("mae", 0)))
        history["val_mae"].append(float(logs.get("val_mae", 0)))

        # Ghi l·∫°i file
        with open(HISTORY_PATH, "w") as f:
            json.dump(history, f, indent=4)


# =======================
# Load model n·∫øu c√≥ checkpoint
# =======================
initial_epoch = 0

if os.path.exists(CHECKPOINT_PATH):
    print("üîÑ Checkpoint detected! Loading model ƒë·ªÉ train ti·∫øp...")
    model = load_model(CHECKPOINT_PATH)

    # L·∫•y epoch hi·ªán t·∫°i t·ª´ file
    if os.path.exists("checkpoints/epoch.txt"):
        with open("checkpoints/epoch.txt", "r") as f:
            initial_epoch = int(f.read())

    print(f"‚ñ∂Ô∏è Ti·∫øp t·ª•c train t·ª´ epoch {initial_epoch}")

else:
    print("üÜï Kh√¥ng c√≥ checkpoint ‚Üí Train m·ªõi t·ª´ ƒë·∫ßu")
    model = build_age_model(X_train.shape[1:])


# =======================
# Callbacks
# =======================
ckpt = ModelCheckpoint(
    CHECKPOINT_PATH,
    monitor="val_loss",
    save_best_only=False,
    save_weights_only=False
)

early = EarlyStopping(monitor="val_loss", patience=8, restore_best_weights=False)
reduce = ReduceLROnPlateau(monitor="val_loss", patience=3, factor=0.2, min_lr=1e-6)


# =======================
# Train (ti·∫øp t·ª•c ho·∫∑c m·ªõi)
# =======================
history = model.fit(
    train_gen,
    validation_data=val_gen,
    initial_epoch=initial_epoch,
    epochs=50,
    callbacks=[ckpt, early, reduce, EpochSaver(), HistorySaver()]
)


# =======================
# V·∫º BI·ªÇU ƒê·ªí GH√âP NHI·ªÄU L·∫¶N TRAIN
# =======================
if os.path.exists(HISTORY_PATH):
    print("üìä ƒêang load to√†n b·ªô history t·ª´ c√°c l·∫ßn train tr∆∞·ªõc...")
    with open(HISTORY_PATH, "r") as f:
        full_history = json.load(f)

    plt.figure(figsize=(10, 4))

    # Loss
    plt.subplot(1, 2, 1)
    plt.plot(full_history["loss"], label="Train Loss")
    plt.plot(full_history["val_loss"], label="Val Loss")
    plt.title("Training Loss")
    plt.xlabel("Epoch")
    plt.ylabel("MSE Loss")
    plt.legend()

    # MAE
    plt.subplot(1, 2, 2)
    plt.plot(full_history["mae"], label="Train MAE")
    plt.plot(full_history["val_mae"], label="Val MAE")
    plt.title("Training MAE")
    plt.xlabel("Epoch")
    plt.ylabel("MAE")
    plt.legend()

    plt.tight_layout()
    plt.savefig("plots/age_training_plot_full.png")
    plt.show()
else:
    print("‚ö† Ch∆∞a c√≥ history.json ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì gh√©p.")


# =======================
# Evaluate Test Set
# =======================
pred = model.predict(X_test).flatten()
mae = mean_absolute_error(y_test, pred)
print("MAE tu·ªïi:", mae)

